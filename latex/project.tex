\documentclass[11pt]{article}
\usepackage{common}
\usepackage{multicol}
\usepackage{amsmath}

\title{Generating Startup Ideas with Markov Chains}
\author{
  \noindent\makebox[0.57\textwidth][l]{\textbf{Colton Gyulay}} \hfill
  \texttt{cgyulay@college.harvard.edu}\\
  \textbf{Antuan Tran} \hfill
  \texttt{antuantran@college.harvard.edu}\\
  \textbf{Evan Gastman} \hfill
  \texttt{evangastman@college.harvard.edu}
}
\begin{document}
\maketitle{}

\begin{multicols}{2}
\section{Introduction}

Language modeling and text generation are important aspects of the greater
field of natural language processing. In this paper, we use a naive language
model based on Markovian assumptions to generate novel sentences from a text
corpus. A major pain point for engineers and entrepreneurs lies in trying to
determine what project or idea to work on. The specific application of text
generation we examine in this paper seeks to alleviate this problem: our model creates fake, believable startup ideas using a dataset of company descriptions from the startup database Crunchbase.

Our generative model is primarily based on Markov chains, which are decision
processes that are characterized by the Markov property. A decision process is said to have the Markov property when any given state is independent of the preceding and following states. We can express this more formally:
\begin{multline}
  P(X_n = x|X_0,\dots,X_{n-1}) =\\
  P(X_n = x|X_{n-1})\,\,\forall{n}\forall{x}
\end{multline}
Though the traditional form of the Markov chain was first produced by mathematician Andrey Markov, the research our implementation follows is
provided by Alexander Volfovsky, 2007.\cite{volfovsky2007} When generating sentences, the biggest difficulties we had were ensuring syntactical correctness and ensuring a desired level of creativity (i.e., not creating sentences that in large part already appear in the training corpus). We were able to dramatically improve syntax by accounting for parts of speech during training and sentence generation. Part-of-speech tagging was done using a maximum entropy model
trained on the University of Pennsylvania's Treebank corpus.\cite{ratnaparkhi1996} To ensure creativity, we used a bag-of-words model to prevent overlap beyond a certain threshhold between generated text and individual sentences in the corpus. The Model section provides a deeper discussion of these systems.

Choosing the correct state size (i.e., the number of tokens in a preceding state) was a critical part of producing strong results. A drawback of using Markov chains in language modeling is that they lack the ability to account for long-term dependencies in the text. For example, a subject and verb that are far apart\textemdash a greater distance that the state size, to be exact\textemdash may not agree in number. We found the ideal state size to be 3 or 4. When choosing a longer state size, these errors tend to disappear as generated text mirrors training text more closely. A shorter state size, however, confers the benefit of greater creativity. In the case of creating startup ideas, the more creative and whimsical, the better.

Using Markov chains for text modeling and generation fell in line nicely with CS 182's study of Markov decision processes and other probabilistic state transition models. Though deterministic in nature, our model was able to create novel and funny startup ideas\textemdash some of which we may be pitching to investors soon.

\section{Related Work}

The basics of Markov chains were explored through two primary sources: Alexander Volfovsky's paper on Markov chains and applications\cite{volfovsky2007} and James Norris' book \textit{Markov Chains}.\cite{norris1998} While Norris provided a more fundamental look at decision processes, focusing on the transition model and studying the underlying probability distribution, Volfovsky supplied a more accessible explanation in regard to application.

The most relevant work came from Grzegorz Szymanski and Zygmnut Ciota, who used hidden Markov models to build transition probabilities, and these HMMs to build Markov chains for generating text.\cite{szymanski2014}

\section{Model}

A clear specification of the algorithm(s) you used and a description
of the main data structures in the implementation. Include a
discussion of any details of the algorithm that were not in the
published paper(s) that formed the basis of your implementation. A
reader should be able to reconstruct and verify your work from reading
your paper.

\section{Body 2}


\begin{algorithm}
  \begin{algorithmic}
    \Procedure{MyAlgorithm}{$b$}
    \State{$a \gets 10$}
    \EndProcedure{}
  \end{algorithmic}
  \caption{Here is the algorithm.}
\end{algorithm}



\section{Experiments}
Analysis, evaluation, and critique of the algorithm and your
implementation. Include a description of the testing data you used and
a discussion of examples that illustrate major features of your
system. Testing is a critical part of system construction, and the
scope of your testing will be an important component in our
evaluation. Discuss what you learned from the implementation.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    & Score \\
    \midrule
    Approach 1 & \\
    Approach 2 & \\
    \bottomrule
  \end{tabular}
  \caption{Description of the results.}
\end{table}

\subsection{Methods and Models}


\subsection{Results}

 For algorithm-comparison projects: a section reporting empirical comparison results preferably presented graphically.


\subsection{Discussion}


\appendix

\section{Program Trace}

Appendix 1 – A trace of the program showing how it handles key examples or some other demonstration of the program in action.

\section{System Description}

 Appendix 2 – A clear description of how to use your system and how to generate the output you discussed in the write-up and the example transcript in Appendix 1. N.B.: The teaching staff must be able to run your system.

\section{Group Makeup}

 Appendix 3 – A list of each project participant and that
participant’s contributions to the project. If the division of work
varies significantly from the project proposal, provide a brief
explanation.  Your code should be clearly documented. 



\bibliographystyle{plain} 
\bibliography{project}

\end{multicols}
\end{document}
